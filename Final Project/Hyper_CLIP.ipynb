{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFWKR2WZx3M39k+38ZM9Z5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickintj/20242R0136COSE47402/blob/main/Final%20Project/Hyper_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation"
      ],
      "metadata": {
        "id": "p25hCmuas6NG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sometimes, need to restart once."
      ],
      "metadata": {
        "id": "iGWbdjVry48W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "%pip install ftfy regex tqdm\n",
        "%pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwnBb251x3KO",
        "outputId": "0785dbdc-f583-43cc-ee2a-2cae6aaaf542"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --yes\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1xjr4m4o\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1xjr4m4o\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=0cc592bf341813c2291eee51dc7d4df19c0ba1b3b42e83a4264f154fc3c3dbc1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yswbt4tq/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Load the dataset\n",
        "root = os.path.expanduser(\"~/.cache\")\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "\n",
        "\n",
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
        "\n",
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(train)\n",
        "test_features, test_labels = get_features(test)\n",
        "\n",
        "# Perform logistic regression\n",
        "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
        "classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "predictions = classifier.predict(test_features)\n",
        "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
        "print(f\"Accuracy = {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "collapsed": true,
        "id": "K1SKvTFVJeba",
        "outputId": "f658d7f5-ddf2-4466-b37e-c0f5da0b25a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/500 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9be0af21a556>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Calculate the image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9be0af21a556>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mall_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mencode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# NLD -> LND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# LND -> NLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/clip/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "IxLFwiUEdD1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Superclasses\n",
        "hypernym = [\n",
        "    'aquatic_mammals', 'fish', 'flowers', 'food_containers',\n",
        "    'fruit_and_vegetables', 'household_electrical_devices',\n",
        "    'household_furniture', 'insects', 'large_carnivores',\n",
        "    'large_man-made_outdoor_things', 'large_natural_outdoor_scenes',\n",
        "    'large_omnivores_and_herbivores', 'medium_mammals',\n",
        "    'non-insect_invertebrates', 'people', 'reptiles', 'small_mammals',\n",
        "    'trees', 'vehicles'\n",
        "]\n",
        "hyponym = [\n",
        "    ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
        "    ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
        "    ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
        "    ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
        "    ['apple', 'mushroom', 'orange', 'pear', 'sweet_pepper'],\n",
        "    ['clock', 'keyboard', 'lamp', 'telephone', 'television'],\n",
        "    ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
        "    ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
        "    ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
        "    ['bridge', 'castle', 'house', 'road', 'skyscraper'],\n",
        "    ['cloud', 'forest', 'mountain', 'plain', 'sea'],\n",
        "    ['camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo'],\n",
        "    ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
        "    ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
        "    ['baby', 'boy', 'girl', 'man', 'woman'],\n",
        "    ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
        "    ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
        "    ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree', 'willow_tree'],\n",
        "    ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train', 'lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor']\n",
        "]"
      ],
      "metadata": {
        "id": "J0B8Y-BaqjJ6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hypernym_idx(arg):\n",
        "  for idx, i in enumerate(hyponym):\n",
        "    if arg in i:\n",
        "      return idx\n",
        "  else:\n",
        "    raise\n",
        "\n",
        "def get_hypernym(arg):\n",
        "  return hypernym[(get_hypernym_idx(arg))]\n",
        "\n",
        "def get_hyponym(arg):\n",
        "  return hyponym[hypernym.index(arg)]"
      ],
      "metadata": {
        "id": "ZOHkBrvLrLRY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Load the dataset\n",
        "root = os.path.expanduser(\"~/.cache\")\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu51dnTKez_p",
        "outputId": "73aee1d8-db9e-496b-cb0e-caa6a40d1e93"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train\n",
        "del test"
      ],
      "metadata": {
        "id": "CFlgyhkvbXkW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
      ],
      "metadata": {
        "id": "Gs3bOCNf0mv0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_hyper(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        idx=0\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in hypernym]).to(device)\n",
        "        labelled=model.encode_text(text_inputs)\n",
        "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labelled)\n",
        "\n",
        "            idx+=1\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
      ],
      "metadata": {
        "id": "SeIEoGePHQTD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_descriptions = [f\"a photo of a {label}\" for label in hypernym]\n",
        "text_tokens = clip.tokenize(text_descriptions).to(device)"
      ],
      "metadata": {
        "id": "0VCedCMtCCB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hyper[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FomVuiDD_jgu",
        "outputId": "9d380581-b698-471e-bf35-7594eba8d2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=32x32>, 19)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(train)\n",
        "test_features, test_labels = get_features(test)\n",
        "\n",
        "# Perform logistic regression\n",
        "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
        "classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "predictions = classifier.predict(test_features)\n",
        "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
        "print(f\"Accuracy = {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "z8HXi9eowKGi",
        "outputId": "802dcfd7-950b-4824-8e9b-8cb7e3fa3c87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a74430e7de1c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Perform logistic regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.classes[train[0][1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aAd0tJJM0vxm",
        "outputId": "0623b506-17ef-4b8d-9a8c-c9499704ca49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cattle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ?"
      ],
      "metadata": {
        "id": "ywH2-hpMtAqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "train_size = int(0.8 * len(train))\n",
        "val_size = len(train) - train_size\n",
        "train_dataset, val_dataset = random_split(train, [train_size, val_size])"
      ],
      "metadata": {
        "id": "V9-38SPkOrYp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset[0][0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcPi07iuZ1i8",
        "outputId": "5b48873f-2394-4a5f-92d5-b22678a0e285"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "224"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OpOjaMp-RL5k",
        "outputId": "a9a5babf-fd86-48f9-b09e-0f1a4e2b0370"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on CLIP in module clip.model object:\n",
            "\n",
            "class CLIP(torch.nn.modules.module.Module)\n",
            " |  CLIP(embed_dim: int, image_resolution: int, vision_layers: Union[Tuple[int, int, int, int], int], vision_width: int, vision_patch_size: int, context_length: int, vocab_size: int, transformer_width: int, transformer_heads: int, transformer_layers: int)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CLIP\n",
            " |      torch.nn.modules.module.Module\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, embed_dim: int, image_resolution: int, vision_layers: Union[Tuple[int, int, int, int], int], vision_width: int, vision_patch_size: int, context_length: int, vocab_size: int, transformer_width: int, transformer_heads: int, transformer_layers: int)\n",
            " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
            " |  \n",
            " |  build_attention_mask(self)\n",
            " |  \n",
            " |  encode_image(self, image)\n",
            " |  \n",
            " |  encode_text(self, text)\n",
            " |  \n",
            " |  forward(self, image, text)\n",
            " |      Define the computation performed at every call.\n",
            " |      \n",
            " |      Should be overridden by all subclasses.\n",
            " |      \n",
            " |      .. note::\n",
            " |          Although the recipe for forward pass needs to be defined within\n",
            " |          this function, one should call the :class:`Module` instance afterwards\n",
            " |          instead of this since the former takes care of running the\n",
            " |          registered hooks while the latter silently ignores them.\n",
            " |  \n",
            " |  initialize_parameters(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  dtype\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Any\n",
            " |      # On the return type:\n",
            " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
            " |      # This is done for better interop with various type checkers for the end users.\n",
            " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
            " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
            " |      # See full discussion on the problems with returning `Union` here\n",
            " |      # https://github.com/microsoft/pyright/issues/4213\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Add a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
            " |      \n",
            " |      Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`nn-init-doc`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> @torch.no_grad()\n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  bfloat16(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
            " |      Return an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf), buf.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  compile(self, *args, **kwargs)\n",
            " |      Compile this Module's forward using :func:`torch.compile`.\n",
            " |      \n",
            " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
            " |      to :func:`torch.compile`.\n",
            " |      \n",
            " |      See :func:`torch.compile` for details on the arguments for this function.\n",
            " |  \n",
            " |  cpu(self: ~T) -> ~T\n",
            " |      Move all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self: ~T) -> ~T\n",
            " |      Set the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |      Set the extra representation of the module.\n",
            " |      \n",
            " |      To print customized extra information, you should re-implement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  float(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  get_buffer(self, target: str) -> 'Tensor'\n",
            " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the buffer\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.Tensor: The buffer referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not a\n",
            " |              buffer\n",
            " |  \n",
            " |  get_extra_state(self) -> Any\n",
            " |      Return any extra state to include in the module's state_dict.\n",
            " |      \n",
            " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
            " |      if you need to store extra state. This function is called when building the\n",
            " |      module's `state_dict()`.\n",
            " |      \n",
            " |      Note that extra state should be picklable to ensure working serialization\n",
            " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
            " |      for serializing Tensors; other objects may break backwards compatibility if\n",
            " |      their serialized pickled form changes.\n",
            " |      \n",
            " |      Returns:\n",
            " |          object: Any extra state to store in the module's state_dict\n",
            " |  \n",
            " |  get_parameter(self, target: str) -> 'Parameter'\n",
            " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the Parameter\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Parameter``\n",
            " |  \n",
            " |  get_submodule(self, target: str) -> 'Module'\n",
            " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To check whether or not we have the ``linear`` submodule, we\n",
            " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
            " |      we have the ``conv`` submodule, we would call\n",
            " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
            " |      \n",
            " |      The runtime of ``get_submodule`` is bounded by the degree\n",
            " |      of module nesting in ``target``. A query against\n",
            " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
            " |      the number of transitive modules. So, for a simple check to see\n",
            " |      if some submodule exists, ``get_submodule`` should always be\n",
            " |      used.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Module: The submodule referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  half(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the IPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on IPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
            " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
            " |      \n",
            " |      If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
            " |          the call to :attr:`load_state_dict` unless\n",
            " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
            " |      \n",
            " |      Args:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
            " |              in the current module are preserved while when ``True``, the\n",
            " |              properties of the Tensors in the state dict are preserved. The only\n",
            " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
            " |              for which the value from the module is preserved.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing any keys that are expected\n",
            " |                  by this module but missing from the provided ``state_dict``.\n",
            " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
            " |                  expected by this module but present in the provided ``state_dict``.\n",
            " |      \n",
            " |      Note:\n",
            " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
            " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
            " |          ``RuntimeError``.\n",
            " |  \n",
            " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the MTIA.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on MTIA while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool, optional): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module. Defaults to True.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>     if name in ['running_var']:\n",
            " |          >>>         print(buf.size())\n",
            " |  \n",
            " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          memo: a memo to store the set of modules already added to the result\n",
            " |          prefix: a prefix that will be added to the name of the module\n",
            " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
            " |              or not\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
            " |              parameters in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>     if name in ['bias']:\n",
            " |          >>>         print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            " |      Return an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param), param.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
            " |      the behavior of this function will change in future versions.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
            " |      Add a buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the module's state. Buffers, by\n",
            " |      default, are persistent and will be saved alongside parameters. This\n",
            " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
            " |      only difference between a persistent buffer and a non-persistent buffer\n",
            " |      is that the latter will not be a part of this module's\n",
            " |      :attr:`state_dict`.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
            " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
            " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
            " |          persistent (bool): whether the buffer is part of this module's\n",
            " |              :attr:`state_dict`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      output. It can modify the input inplace but it will not have effect on\n",
            " |      forward since this is called after :func:`forward` is called. The hook\n",
            " |      should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, output) -> None or modified output\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
            " |      ``kwargs`` given to the forward function and be expected to return the\n",
            " |      output possibly modified. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs, output) -> None or modified output\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
            " |              before all existing ``forward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward`` hooks registered with\n",
            " |              :func:`register_module_forward_hook` will fire before all hooks\n",
            " |              registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
            " |              kwargs given to the forward function.\n",
            " |              Default: ``False``\n",
            " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
            " |              whether an exception is raised while calling the Module.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      \n",
            " |      \n",
            " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      input. User can either return a tuple or a single modified value in the\n",
            " |      hook. We will wrap the value into a tuple if a single value is returned\n",
            " |      (unless that value is already a tuple). The hook should have the\n",
            " |      following signature::\n",
            " |      \n",
            " |          hook(module, args) -> None or modified input\n",
            " |      \n",
            " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
            " |      kwargs given to the forward function. And if the hook modifies the\n",
            " |      input, both the args and kwargs should be returned. The hook should have\n",
            " |      the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``forward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward_pre`` hooks registered with\n",
            " |              :func:`register_module_forward_pre_hook` will fire before all\n",
            " |              hooks registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
            " |              given to the forward function.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to a module\n",
            " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
            " |      respect to module outputs are computed. The hook should have the following\n",
            " |      signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
            " |      with respect to the inputs and outputs respectively. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
            " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
            " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
            " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
            " |      arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward`` hooks registered with\n",
            " |              :func:`register_module_full_backward_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients for the module are computed.\n",
            " |      The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
            " |      \n",
            " |      The :attr:`grad_output` is a tuple. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
            " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
            " |      all non-Tensor arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward_pre`` hooks registered with\n",
            " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_load_state_dict_post_hook(self, hook)\n",
            " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, incompatible_keys) -> None\n",
            " |      \n",
            " |      The ``module`` argument is the current module that this hook is registered\n",
            " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
            " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
            " |      is a ``list`` of ``str`` containing the missing keys and\n",
            " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
            " |      \n",
            " |      The given incompatible_keys can be modified inplace if needed.\n",
            " |      \n",
            " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
            " |      ``strict=True`` are affected by modifications the hook makes to\n",
            " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
            " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
            " |      clearing out both missing and unexpected keys will avoid an error.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_load_state_dict_pre_hook(self, hook)\n",
            " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
            " |      \n",
            " |      Arguments:\n",
            " |          hook (Callable): Callable hook that will be invoked before\n",
            " |              loading the state dict.\n",
            " |  \n",
            " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Alias for :func:`add_module`.\n",
            " |  \n",
            " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
            " |      Add a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter or None): parameter to be added to the module. If\n",
            " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
            " |              are ignored. If ``None``, the parameter is **not** included in the\n",
            " |              module's :attr:`state_dict`.\n",
            " |  \n",
            " |  register_state_dict_post_hook(self, hook)\n",
            " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
            " |      \n",
            " |      The registered hooks can modify the ``state_dict`` inplace.\n",
            " |  \n",
            " |  register_state_dict_pre_hook(self, hook)\n",
            " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, prefix, keep_vars) -> None\n",
            " |      \n",
            " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
            " |      call is made.\n",
            " |  \n",
            " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
            " |      Change if autograd should record operations on parameters in this module.\n",
            " |      \n",
            " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
            " |      in-place.\n",
            " |      \n",
            " |      This method is helpful for freezing part of the module for finetuning\n",
            " |      or training parts of a model individually (e.g., GAN training).\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): whether autograd should record operations on\n",
            " |                                parameters in this module. Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  set_extra_state(self, state: Any) -> None\n",
            " |      Set extra state contained in the loaded `state_dict`.\n",
            " |      \n",
            " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
            " |      found within the `state_dict`. Implement this function and a corresponding\n",
            " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
            " |      `state_dict`.\n",
            " |      \n",
            " |      Args:\n",
            " |          state (dict): Extra state from the `state_dict`\n",
            " |  \n",
            " |  set_submodule(self, target: str, module: 'Module') -> None\n",
            " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To overide the ``Conv2d`` with a new submodule ``Linear``, you\n",
            " |      would call\n",
            " |      ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |          module: The module to set the submodule to.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the target string is empty\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  share_memory(self: ~T) -> ~T\n",
            " |      See :meth:`torch.Tensor.share_memory_`.\n",
            " |  \n",
            " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
            " |      Return a dictionary containing references to the whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      Parameters and buffers set to ``None`` are not included.\n",
            " |      \n",
            " |      .. note::\n",
            " |          The returned object is a shallow copy. It contains references\n",
            " |          to the module's parameters and buffers.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Currently ``state_dict()`` also accepts positional arguments for\n",
            " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
            " |          this is being deprecated and keyword arguments will be enforced in\n",
            " |          future releases.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Please avoid the use of argument ``destination`` as it is not\n",
            " |          designed for end-users.\n",
            " |      \n",
            " |      Args:\n",
            " |          destination (dict, optional): If provided, the state of module will\n",
            " |              be updated into the dict and the same object is returned.\n",
            " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
            " |              Default: ``None``.\n",
            " |          prefix (str, optional): a prefix added to parameter and buffer\n",
            " |              names to compose the keys in state_dict. Default: ``''``.\n",
            " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
            " |              returned in the state dict are detached from autograd. If it's\n",
            " |              set to ``True``, detaching will not be performed.\n",
            " |              Default: ``False``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Move and/or cast the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(memory_format=torch.channels_last)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
            " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
            " |              the parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
            " |              format for 4D parameters and buffers in this module (keyword\n",
            " |              only argument)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
            " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
            " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
            " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
            " |  \n",
            " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
            " |      Move the parameters and buffers to the specified device without copying storage.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The desired device of the parameters\n",
            " |              and buffers in this module.\n",
            " |          recurse (bool): Whether parameters and buffers of submodules should\n",
            " |              be recursively moved to the specified device.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  train(self: ~T, mode: bool = True) -> ~T\n",
            " |      Set the module in training mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
            " |                       mode (``False``). Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the XPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on XPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self, set_to_none: bool = True) -> None\n",
            " |      Reset gradients of all model parameters.\n",
            " |      \n",
            " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
            " |      \n",
            " |      Args:\n",
            " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
            " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  T_destination = ~T_destination\n",
            " |  \n",
            " |  call_super_init = False\n",
            " |  \n",
            " |  dump_patches = False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "vKZdXx4pO6D6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    pbar = tqdm(train_loader, total=len(train_loader))\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images,texts = batch\n",
        "\n",
        "        images= images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        texts=clip.tokenize(hypernym).to(device)\n",
        "        print(texts.shape)\n",
        "\n",
        "        # Forward pass\n",
        "\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute loss\n",
        "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "        total_loss = loss_img(logits_per_image,ground_truth)/2\n",
        "        total_loss+=loss_txt(logits_per_text,ground_truth)/2\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch}/{num_epochs}, Loss: {total_loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1BxI4CcXDN8",
        "outputId": "7014eb02-fdd9-418c-bd5b-88c373d267e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19, 77])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def loss_fn(logits_per_image, logits_per_text):\n",
        "    assert logits_per_image.shape[0] == logits_per_image.shape[0] # logits' shape should be (nxn)\n",
        "    assert logits_per_image.shape == logits_per_text.shape\n",
        "\n",
        "    labels = torch.arange(logits_per_image.shape[0], device=device)\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)\n",
        "    loss_t = F.cross_entropy(logits_per_image, labels)\n",
        "    loss = (loss_i + loss_t) / 2\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "8gWjoXUjcVXl"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}